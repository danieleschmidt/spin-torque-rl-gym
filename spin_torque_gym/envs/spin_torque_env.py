"""Main Gymnasium environment for spintronic device control.

This module implements the core RL environment where agents learn to control
magnetization switching in spintronic devices through current pulses.
"""

import gymnasium as gym
from gymnasium import spaces
import numpy as np
from typing import Dict, Any, Optional, Tuple, Union, List
import warnings

from ..physics import LLGSSolver, ThermalFluctuations, MaterialDatabase
from ..devices import DeviceFactory
from ..rewards import CompositeReward


class SpinTorqueEnv(gym.Env):
    """Gymnasium environment for spintronic device control via RL.
    
    The agent learns to apply current pulses to switch magnetization
    from initial to target states while minimizing energy consumption
    and maximizing switching reliability.
    """
    
    metadata = {'render_modes': ['human', 'rgb_array'], 'render_fps': 30}
    
    def __init__(
        self,
        device_type: str = 'stt_mram',
        device_params: Optional[Dict[str, Any]] = None,
        target_states: Optional[List[np.ndarray]] = None,
        max_steps: int = 100,
        max_current: float = 2e6,  # A/m²
        max_duration: float = 5e-9,  # 5 ns
        temperature: float = 300.0,  # K
        include_thermal_fluctuations: bool = True,
        reward_components: Optional[Dict[str, Dict]] = None,
        action_mode: str = 'continuous',
        observation_mode: str = 'vector',
        success_threshold: float = 0.9,
        energy_penalty_weight: float = 0.1,
        render_mode: Optional[str] = None,
        seed: Optional[int] = None
    ):
        """Initialize SpinTorque environment.
        
        Args:
            device_type: Type of device ('stt_mram', 'sot_mram', 'vcma_mram')
            device_params: Custom device parameters
            target_states: List of possible target magnetization states
            max_steps: Maximum steps per episode
            max_current: Maximum current density magnitude (A/m²)
            max_duration: Maximum pulse duration (s)
            temperature: Operating temperature (K)
            include_thermal_fluctuations: Whether to include thermal noise
            reward_components: Custom reward function components
            action_mode: 'continuous' or 'discrete'
            observation_mode: 'vector' or 'dict'
            success_threshold: Threshold for successful switching (dot product)
            energy_penalty_weight: Weight for energy penalty in reward
            render_mode: Rendering mode ('human', 'rgb_array', None)
            seed: Random seed for reproducibility
        """\n        super().__init__()\n        \n        # Environment configuration\n        self.device_type = device_type\n        self.max_steps = max_steps\n        self.max_current = max_current\n        self.max_duration = max_duration\n        self.temperature = temperature\n        self.include_thermal = include_thermal_fluctuations\n        self.action_mode = action_mode\n        self.observation_mode = observation_mode\n        self.success_threshold = success_threshold\n        self.energy_penalty_weight = energy_penalty_weight\n        self.render_mode = render_mode\n        \n        # Initialize random number generator\n        self._np_random = None\n        self.seed(seed)\n        \n        # Initialize physics components\n        self.solver = LLGSSolver(method='RK45', rtol=1e-6, atol=1e-9)\n        self.thermal_model = ThermalFluctuations(\n            temperature=temperature,\n            correlation_time=1e-12,\n            seed=seed\n        )\n        self.material_db = MaterialDatabase()\n        \n        # Initialize device\n        device_factory = DeviceFactory()\n        if device_params is None:\n            device_params = self._get_default_device_params()\n        self.device = device_factory.create_device(device_type, device_params)\n        \n        # Set target states\n        if target_states is None:\n            self.target_states = [np.array([0, 0, 1]), np.array([0, 0, -1])]  # ±z\n        else:\n            self.target_states = [self.device.validate_magnetization(t) for t in target_states]\n        \n        # Initialize reward function\n        if reward_components is None:\n            reward_components = self._get_default_reward_components()\n        self.reward_function = CompositeReward(reward_components)\n        \n        # Define action space\n        self._setup_action_space()\n        \n        # Define observation space\n        self._setup_observation_space()\n        \n        # Environment state\n        self.current_magnetization = None\n        self.target_magnetization = None\n        self.step_count = 0\n        self.total_energy = 0.0\n        self.episode_history = []\n        self.last_action = np.zeros(self.action_space.shape)\n        \n        # Rendering\n        self.renderer = None\n        if render_mode == 'human':\n            self._init_renderer()\n    \n    def _get_default_device_params(self) -> Dict[str, Any]:\n        \"\"\"Get default device parameters based on device type.\"\"\"\n        if self.device_type == 'stt_mram':\n            return {\n                'volume': 50e-9 * 100e-9 * 2e-9,  # 50×100×2 nm³\n                'area': 50e-9 * 100e-9,\n                'thickness': 2e-9,\n                'aspect_ratio': 2.0,\n                'saturation_magnetization': 800e3,  # A/m\n                'damping': 0.01,\n                'uniaxial_anisotropy': 1.2e6,  # J/m³\n                'exchange_constant': 20e-12,  # J/m\n                'polarization': 0.7,\n                'resistance_parallel': 1e3,  # Ω\n                'resistance_antiparallel': 2e3,  # Ω\n                'easy_axis': np.array([0, 0, 1]),\n                'reference_magnetization': np.array([0, 0, 1])\n            }\n        else:\n            # Default parameters for other device types\n            return {\n                'volume': 1e-24,\n                'saturation_magnetization': 800e3,\n                'damping': 0.01,\n                'uniaxial_anisotropy': 1e6,\n                'polarization': 0.7\n            }\n    \n    def _get_default_reward_components(self) -> Dict[str, Dict]:\n        \"\"\"Get default reward function components.\"\"\"\n        return {\n            'success': {\n                'weight': 10.0,\n                'function': lambda obs, action, next_obs, info: \n                    10.0 if info.get('is_success', False) else 0.0\n            },\n            'energy': {\n                'weight': -self.energy_penalty_weight,\n                'function': lambda obs, action, next_obs, info:\n                    -info.get('step_energy', 0.0) / 1e-12  # Normalize to pJ\n            },\n            'progress': {\n                'weight': 1.0,\n                'function': lambda obs, action, next_obs, info:\n                    info.get('alignment_improvement', 0.0)\n            },\n            'stability': {\n                'weight': -2.0,\n                'function': lambda obs, action, next_obs, info:\n                    -max(0, np.linalg.norm(next_obs['magnetization']) - 1.1)\n            }\n        }\n    \n    def _setup_action_space(self):\n        \"\"\"Setup action space based on action mode.\"\"\"\n        if self.action_mode == 'continuous':\n            # Continuous: [current_density, pulse_duration]\n            self.action_space = spaces.Box(\n                low=np.array([-self.max_current, 0.0]),\n                high=np.array([self.max_current, self.max_duration]),\n                dtype=np.float32\n            )\n        elif self.action_mode == 'discrete':\n            # Discrete: 5 current levels × 4 duration levels = 20 actions\n            self.current_levels = np.linspace(-self.max_current, self.max_current, 5)\n            self.duration_levels = np.array([0.1e-9, 0.5e-9, 1.0e-9, 2.0e-9])\n            self.action_space = spaces.Discrete(len(self.current_levels) * len(self.duration_levels))\n        else:\n            raise ValueError(f\"Unknown action mode: {self.action_mode}\")\n    \n    def _setup_observation_space(self):\n        \"\"\"Setup observation space based on observation mode.\"\"\"\n        if self.observation_mode == 'vector':\n            # Vector observation: [mx, my, mz, tx, ty, tz, R/R0, T/T0, t_rem, E_norm, I_prev, dt_prev]\n            self.observation_space = spaces.Box(\n                low=-np.inf,\n                high=np.inf,\n                shape=(12,),\n                dtype=np.float32\n            )\n        elif self.observation_mode == 'dict':\n            # Dictionary observation\n            self.observation_space = spaces.Dict({\n                'magnetization': spaces.Box(-1, 1, shape=(3,), dtype=np.float32),\n                'target': spaces.Box(-1, 1, shape=(3,), dtype=np.float32),\n                'resistance': spaces.Box(0, np.inf, shape=(1,), dtype=np.float32),\n                'temperature': spaces.Box(0, np.inf, shape=(1,), dtype=np.float32),\n                'steps_remaining': spaces.Box(0, self.max_steps, shape=(1,), dtype=int),\n                'energy_consumed': spaces.Box(0, np.inf, shape=(1,), dtype=np.float32),\n                'last_action': self.action_space\n            })\n        else:\n            raise ValueError(f\"Unknown observation mode: {self.observation_mode}\")\n    \n    def reset(\n        self,\n        seed: Optional[int] = None,\n        options: Optional[Dict[str, Any]] = None\n    ) -> Tuple[Union[np.ndarray, Dict], Dict[str, Any]]:\n        \"\"\"Reset environment to initial state.\n        \n        Args:\n            seed: Random seed\n            options: Reset options\n            \n        Returns:\n            Tuple of (observation, info)\n        \"\"\"\n        super().reset(seed=seed)\n        \n        if options is None:\n            options = {}\n        \n        # Reset environment state\n        self.step_count = 0\n        self.total_energy = 0.0\n        self.episode_history = []\n        self.last_action = np.zeros(self.action_space.shape if hasattr(self.action_space, 'shape') else 2)\n        \n        # Set initial magnetization\n        if 'initial_state' in options:\n            self.current_magnetization = self.device.validate_magnetization(options['initial_state'])\n        else:\n            # Random initial state\n            initial_state = self._np_random.normal(0, 1, 3)\n            self.current_magnetization = self.device.validate_magnetization(initial_state)\n        \n        # Set target magnetization\n        if 'target_state' in options:\n            self.target_magnetization = self.device.validate_magnetization(options['target_state'])\n        else:\n            # Random target from available states\n            self.target_magnetization = self._np_random.choice(self.target_states)\n        \n        # Update thermal model temperature if specified\n        if 'temperature' in options:\n            self.thermal_model.set_temperature(options['temperature'])\n        \n        observation = self._get_observation()\n        info = self._get_info()\n        \n        return observation, info\n    \n    def step(\n        self,\n        action: Union[np.ndarray, int]\n    ) -> Tuple[Union[np.ndarray, Dict], float, bool, bool, Dict[str, Any]]:\n        \"\"\"Execute one environment step.\n        \n        Args:\n            action: Action to execute\n            \n        Returns:\n            Tuple of (observation, reward, terminated, truncated, info)\n        \"\"\"\n        if self.current_magnetization is None:\n            raise RuntimeError(\"Environment must be reset before calling step\")\n        \n        # Parse action\n        current_density, pulse_duration = self._parse_action(action)\n        self.last_action = np.array([current_density, pulse_duration])\n        \n        # Store previous state for reward calculation\n        prev_magnetization = self.current_magnetization.copy()\n        prev_alignment = np.dot(prev_magnetization, self.target_magnetization)\n        \n        # Simulate magnetization dynamics\n        step_info = self._simulate_dynamics(current_density, pulse_duration)\n        \n        # Update state\n        self.current_magnetization = step_info['final_magnetization']\n        self.total_energy += step_info['energy_consumed']\n        self.step_count += 1\n        \n        # Calculate reward\n        current_alignment = np.dot(self.current_magnetization, self.target_magnetization)\n        alignment_improvement = current_alignment - prev_alignment\n        \n        is_success = current_alignment >= self.success_threshold\n        \n        reward_info = {\n            'is_success': is_success,\n            'step_energy': step_info['energy_consumed'],\n            'alignment_improvement': alignment_improvement,\n            'current_alignment': current_alignment\n        }\n        \n        observation = self._get_observation()\n        reward = self.reward_function.compute(None, action, observation, reward_info)\n        \n        # Check termination conditions\n        terminated = is_success\n        truncated = self.step_count >= self.max_steps\n        \n        # Store step in history\n        self.episode_history.append({\n            'step': self.step_count,\n            'action': [current_density, pulse_duration],\n            'magnetization': self.current_magnetization.copy(),\n            'reward': reward,\n            'energy': step_info['energy_consumed'],\n            'alignment': current_alignment\n        })\n        \n        info = self._get_info()\n        info.update(step_info)\n        info.update(reward_info)\n        \n        return observation, reward, terminated, truncated, info\n    \n    def _parse_action(self, action: Union[np.ndarray, int]) -> Tuple[float, float]:\n        \"\"\"Parse action into current density and pulse duration.\"\"\"\n        if self.action_mode == 'continuous':\n            if isinstance(action, (int, float)):\n                # Single value - interpret as current with default duration\n                current_density = float(action)\n                pulse_duration = 1e-9  # Default 1 ns\n            else:\n                current_density = float(action[0])\n                pulse_duration = float(action[1]) if len(action) > 1 else 1e-9\n        elif self.action_mode == 'discrete':\n            action_idx = int(action)\n            current_idx = action_idx // len(self.duration_levels)\n            duration_idx = action_idx % len(self.duration_levels)\n            \n            current_density = self.current_levels[current_idx]\n            pulse_duration = self.duration_levels[duration_idx]\n        else:\n            raise ValueError(f\"Unknown action mode: {self.action_mode}\")\n        \n        # Clip to valid ranges\n        current_density = np.clip(current_density, -self.max_current, self.max_current)\n        pulse_duration = np.clip(pulse_duration, 1e-12, self.max_duration)\n        \n        return current_density, pulse_duration\n    \n    def _simulate_dynamics(\n        self,\n        current_density: float,\n        pulse_duration: float\n    ) -> Dict[str, Any]:\n        \"\"\"Simulate magnetization dynamics for given current pulse.\"\"\"\n        # Define current function\n        def current_func(t: float) -> float:\n            return current_density if t <= pulse_duration else 0.0\n        \n        # Define field function (no external field by default)\n        def field_func(t: float) -> np.ndarray:\n            return np.zeros(3)\n        \n        # Simulate dynamics\n        try:\n            result = self.solver.solve(\n                m_initial=self.current_magnetization,\n                time_span=(0, pulse_duration),\n                device_params=self.device.device_params,\n                current_func=current_func,\n                field_func=field_func,\n                thermal_noise=self.include_thermal,\n                temperature=self.temperature\n            )\n            \n            if result['success']:\n                final_magnetization = result['m'][-1]\n                # Ensure normalization\n                final_magnetization = final_magnetization / np.linalg.norm(final_magnetization)\n            else:\n                warnings.warn(\"Dynamics simulation failed, using initial state\")\n                final_magnetization = self.current_magnetization\n                \n        except Exception as e:\n            warnings.warn(f\"Error in dynamics simulation: {e}\")\n            final_magnetization = self.current_magnetization\n        \n        # Calculate energy consumed\n        if abs(current_density) > 1e-12:\n            resistance = self.device.compute_resistance(self.current_magnetization)\n            area = self.device.get_parameter('area', 1e-14)\n            voltage = current_density * resistance * area\n            energy_consumed = voltage**2 / resistance * pulse_duration\n        else:\n            energy_consumed = 0.0\n        \n        return {\n            'final_magnetization': final_magnetization,\n            'energy_consumed': energy_consumed,\n            'pulse_duration': pulse_duration,\n            'current_density': current_density,\n            'simulation_success': result.get('success', False) if 'result' in locals() else False\n        }\n    \n    def _get_observation(self) -> Union[np.ndarray, Dict]:\n        \"\"\"Get current observation.\"\"\"\n        if self.observation_mode == 'vector':\n            # Compute normalized values\n            resistance = self.device.compute_resistance(self.current_magnetization)\n            r0 = self.device.get_parameter('resistance_parallel', 1e3)\n            \n            temp_norm = self.temperature / 300.0\n            steps_remaining_norm = (self.max_steps - self.step_count) / self.max_steps\n            energy_norm = self.total_energy / 1e-12  # Normalize to pJ\n            \n            current_norm = self.last_action[0] / self.max_current if len(self.last_action) > 0 else 0.0\n            duration_norm = self.last_action[1] / self.max_duration if len(self.last_action) > 1 else 0.0\n            \n            obs = np.array([\n                *self.current_magnetization,  # mx, my, mz\n                *self.target_magnetization,   # tx, ty, tz\n                resistance / r0,              # Normalized resistance\n                temp_norm,                    # Normalized temperature\n                steps_remaining_norm,         # Normalized steps remaining\n                energy_norm,                  # Normalized energy\n                current_norm,                 # Previous current (normalized)\n                duration_norm                 # Previous duration (normalized)\n            ], dtype=np.float32)\n            \n        elif self.observation_mode == 'dict':\n            resistance = self.device.compute_resistance(self.current_magnetization)\n            \n            obs = {\n                'magnetization': self.current_magnetization.astype(np.float32),\n                'target': self.target_magnetization.astype(np.float32),\n                'resistance': np.array([resistance], dtype=np.float32),\n                'temperature': np.array([self.temperature], dtype=np.float32),\n                'steps_remaining': np.array([self.max_steps - self.step_count], dtype=int),\n                'energy_consumed': np.array([self.total_energy], dtype=np.float32),\n                'last_action': self.last_action.astype(np.float32)\n            }\n        \n        return obs\n    \n    def _get_info(self) -> Dict[str, Any]:\n        \"\"\"Get info dictionary.\"\"\"\n        current_alignment = np.dot(self.current_magnetization, self.target_magnetization)\n        \n        return {\n            'step_count': self.step_count,\n            'total_energy': self.total_energy,\n            'current_alignment': current_alignment,\n            'is_success': current_alignment >= self.success_threshold,\n            'target_reached': current_alignment >= self.success_threshold,\n            'magnetization_magnitude': np.linalg.norm(self.current_magnetization),\n            'device_type': self.device_type,\n            'episode_history': self.episode_history.copy()\n        }\n    \n    def render(self, mode: Optional[str] = None):\n        \"\"\"Render the environment.\"\"\"\n        if mode is None:\n            mode = self.render_mode\n        \n        if mode == 'human':\n            self._render_human()\n        elif mode == 'rgb_array':\n            return self._render_rgb_array()\n        elif mode is None:\n            return\n        else:\n            raise ValueError(f\"Unsupported render mode: {mode}\")\n    \n    def _init_renderer(self):\n        \"\"\"Initialize renderer for human mode.\"\"\"\n        try:\n            import matplotlib.pyplot as plt\n            from mpl_toolkits.mplot3d import Axes3D\n            \n            self.fig = plt.figure(figsize=(12, 8))\n            self.ax_3d = self.fig.add_subplot(221, projection='3d')\n            self.ax_energy = self.fig.add_subplot(222)\n            self.ax_alignment = self.fig.add_subplot(223)\n            self.ax_action = self.fig.add_subplot(224)\n            \n            plt.ion()  # Interactive mode\n            self.renderer = True\n            \n        except ImportError:\n            warnings.warn(\"Matplotlib not available, rendering disabled\")\n            self.renderer = None\n    \n    def _render_human(self):\n        \"\"\"Render for human viewing.\"\"\"\n        if self.renderer is None:\n            return\n        \n        try:\n            import matplotlib.pyplot as plt\n            \n            # Clear axes\n            self.ax_3d.clear()\n            self.ax_energy.clear()\n            self.ax_alignment.clear()\n            self.ax_action.clear()\n            \n            # 3D magnetization visualization\n            self.ax_3d.quiver(0, 0, 0, *self.current_magnetization, color='red', label='Current', arrow_length_ratio=0.1)\n            self.ax_3d.quiver(0, 0, 0, *self.target_magnetization, color='blue', label='Target', arrow_length_ratio=0.1)\n            \n            # Draw unit sphere\n            u = np.linspace(0, 2 * np.pi, 50)\n            v = np.linspace(0, np.pi, 50)\n            x_sphere = np.outer(np.cos(u), np.sin(v))\n            y_sphere = np.outer(np.sin(u), np.sin(v))\n            z_sphere = np.outer(np.ones(np.size(u)), np.cos(v))\n            self.ax_3d.plot_surface(x_sphere, y_sphere, z_sphere, alpha=0.1, color='gray')\n            \n            self.ax_3d.set_xlim([-1.5, 1.5])\n            self.ax_3d.set_ylim([-1.5, 1.5])\n            self.ax_3d.set_zlim([-1.5, 1.5])\n            self.ax_3d.set_xlabel('X')\n            self.ax_3d.set_ylabel('Y')\n            self.ax_3d.set_zlabel('Z')\n            self.ax_3d.legend()\n            self.ax_3d.set_title('Magnetization State')\n            \n            # Plot energy and alignment history if available\n            if self.episode_history:\n                steps = [h['step'] for h in self.episode_history]\n                energies = [h['energy'] for h in self.episode_history]\n                alignments = [h['alignment'] for h in self.episode_history]\n                currents = [h['action'][0] for h in self.episode_history]\n                \n                self.ax_energy.plot(steps, energies, 'g-')\n                self.ax_energy.set_xlabel('Step')\n                self.ax_energy.set_ylabel('Energy (J)')\n                self.ax_energy.set_title('Energy Consumption')\n                \n                self.ax_alignment.plot(steps, alignments, 'b-')\n                self.ax_alignment.axhline(y=self.success_threshold, color='r', linestyle='--', label='Success threshold')\n                self.ax_alignment.set_xlabel('Step')\n                self.ax_alignment.set_ylabel('Alignment')\n                self.ax_alignment.set_title('Target Alignment')\n                self.ax_alignment.legend()\n                \n                self.ax_action.plot(steps, currents, 'orange')\n                self.ax_action.set_xlabel('Step')\n                self.ax_action.set_ylabel('Current (A/m²)')\n                self.ax_action.set_title('Applied Current')\n            \n            plt.tight_layout()\n            plt.draw()\n            plt.pause(0.01)\n            \n        except Exception as e:\n            warnings.warn(f\"Rendering error: {e}\")\n    \n    def _render_rgb_array(self) -> np.ndarray:\n        \"\"\"Render as RGB array.\"\"\"\n        # Simplified rendering - return a placeholder image\n        import matplotlib.pyplot as plt\n        \n        fig, ax = plt.subplots(figsize=(8, 6))\n        \n        # Plot magnetization state\n        ax.quiver(0, 0, self.current_magnetization[0], self.current_magnetization[1], \n                 color='red', scale=1, label='Current')\n        ax.quiver(0, 0, self.target_magnetization[0], self.target_magnetization[1], \n                 color='blue', scale=1, label='Target')\n        \n        # Draw unit circle\n        circle = plt.Circle((0, 0), 1, fill=False, color='gray', alpha=0.5)\n        ax.add_patch(circle)\n        \n        ax.set_xlim([-1.5, 1.5])\n        ax.set_ylim([-1.5, 1.5])\n        ax.set_aspect('equal')\n        ax.legend()\n        ax.set_title(f'Step {self.step_count}: Alignment = {np.dot(self.current_magnetization, self.target_magnetization):.3f}')\n        \n        # Convert to RGB array\n        fig.canvas.draw()\n        rgb_array = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n        rgb_array = rgb_array.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n        \n        plt.close(fig)\n        return rgb_array\n    \n    def close(self):\n        \"\"\"Close environment and cleanup.\"\"\"\n        if hasattr(self, 'fig'):\n            import matplotlib.pyplot as plt\n            plt.close(self.fig)\n        \n        self.renderer = None\n    \n    def seed(self, seed: Optional[int] = None) -> List[int]:\n        \"\"\"Set random seed.\"\"\"\n        self._np_random, seed = gym.utils.seeding.np_random(seed)\n        return [seed]\n    \n    def get_device_info(self) -> Dict[str, Any]:\n        \"\"\"Get device information.\"\"\"\n        return self.device.get_device_info()\n    \n    def analyze_episode(self) -> Dict[str, Any]:\n        \"\"\"Analyze completed episode.\"\"\"\n        if not self.episode_history:\n            return {}\n        \n        total_energy = sum(h['energy'] for h in self.episode_history)\n        final_alignment = self.episode_history[-1]['alignment']\n        success = final_alignment >= self.success_threshold\n        \n        # Calculate switching time (first time alignment exceeds threshold)\n        switching_step = None\n        for i, h in enumerate(self.episode_history):\n            if h['alignment'] >= self.success_threshold:\n                switching_step = i + 1\n                break\n        \n        return {\n            'episode_length': len(self.episode_history),\n            'total_energy': total_energy,\n            'final_alignment': final_alignment,\n            'success': success,\n            'switching_step': switching_step,\n            'average_reward': np.mean([h['reward'] for h in self.episode_history]),\n            'energy_efficiency': final_alignment / total_energy if total_energy > 0 else 0,\n            'history': self.episode_history.copy()\n        }